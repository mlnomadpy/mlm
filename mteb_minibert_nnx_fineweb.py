# -*- coding: utf-8 -*-
"""MTEB MiniBERT NNX - Fineweb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/skywolfmo/mteb-minibert-nnx-fineweb.e18e06ad-0958-480f-8333-10ca371b0d98.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250913/auto/storage/goog4_request%26X-Goog-Date%3D20250913T024830Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3D183529a92dc350e2b2a621895c424ffc8ff9bf867a617bdf3d09ce2ce90c8771d245a201a69fba95b563343766ff95561b26c18ea1b13d3d7a1d53a08a6f3767a6409ed33d74e90b08ae16fd93040a35c2ce16d248b88bd30d7815d31a92a4f87499d51464e232b20a9c676cd70a5b6ce7e9456edf976495ae723c81970cdc7cecbf9e1e6ec71cc1666a26daeac8734c5e9cbed0a10c9637d52b49542d72e3f7687ef78ab5ee9f1adc6302e030c680fe5e53beca28bc526620515ef350314907231faba417cbf8958d1d922df4b64cf51a2585e5dad3d2054a9b0502dc01f5f087803dfe3864ba2baf56a147b631e01a29ce67a0df0d833fb6239cb158575011
"""

!pip install -Uq tiktoken datasets wandb mteb

from kaggle_secrets import UserSecretsClient
import os
user_secrets = UserSecretsClient()
os.environ["HF_TOKEN"] = user_secrets.get_secret("HF_TOKEN")
os.environ["WANDB_API_KEY"] = user_secrets.get_secret("WANDB_API_KEY")
os.environ["WANDB_ENTITY"] = user_secrets.get_secret("WANDB_ENTITY")

!rm -rf /kaggle/working/minibert_checkpoints

# Install necessary libraries
# The following command ensures JAX is installed correctly for a TPU environment,
# which is critical to avoid XlaRuntimeError. It also installs/upgrades other dependencies.
# !pip install -Uq "jax[tpu]" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
# !pip install -Uq tiktoken datasets wandb orbax-checkpoint flax optax mteb torch

import os
import time
import json
from dataclasses import dataclass

import flax.nnx as nnx
import jax
import jax.numpy as jnp
import numpy as np
import optax
import orbax.checkpoint as orbax
import tiktoken
import wandb
from datasets import load_dataset
from jax.experimental import mesh_utils
from jax.sharding import Mesh, NamedSharding
from jax.sharding import PartitionSpec as P
from mteb import MTEB

# --- JAX Device and Mesh Setup ---
# Initialize JAX devices
if jax.default_backend() == 'tpu':
    mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))
else:
    num_devices = len(jax.devices())
    mesh_shape = (num_devices, 1)
    mesh = Mesh(mesh_utils.create_device_mesh(mesh_shape), ('batch', 'model'))

# --- Tokenizer ---
tokenizer = tiktoken.get_encoding("gpt2")
MASK_TOKEN_ID = tokenizer.n_vocab


# --- Model Definition ---
class TransformerBlock(nnx.Module):
    """A standard Transformer block with bidirectional self-attention."""
    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):
        if mesh is not None:
            kernel_init = nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model')))
        else:
            kernel_init = nnx.initializers.xavier_uniform()

        self.mha = nnx.MultiHeadAttention(num_heads=num_heads, in_features=embed_dim, use_bias=False, kernel_init=kernel_init, rngs=rngs)
        self.dropout1 = nnx.Dropout(rate=rate, rngs=rngs)
        self.ffn = nnx.Sequential(
            nnx.Linear(embed_dim, ff_dim, kernel_init=kernel_init, rngs=rngs),
            nnx.gelu,
            nnx.Linear(ff_dim, embed_dim, kernel_init=kernel_init, rngs=rngs)
        )
        self.dropout2 = nnx.Dropout(rate=rate, rngs=rngs)
        self.norm1 = nnx.LayerNorm(embed_dim, rngs=rngs)
        self.norm2 = nnx.LayerNorm(embed_dim, rngs=rngs)

    def __call__(self, inputs, training: bool = False):
        x = self.norm1(inputs)
        attention_output = self.mha(inputs_q=x, decode=False)
        attention_output = self.dropout1(attention_output, deterministic=not training)
        out1 = inputs + attention_output
        x = self.norm2(out1)
        ffn_output = self.ffn(x)
        ffn_output = self.dropout2(ffn_output, deterministic=not training)
        return out1 + ffn_output

class TokenAndPositionEmbedding(nnx.Module):
    """Combines token and positional embeddings."""
    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):
        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)
        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)

    def __call__(self, x):
        positions = jnp.arange(0, x.shape[1])[None, :]
        return self.token_emb(x) + self.pos_emb(positions)

class MiniBERT(nnx.Module):
    """A miniBERT transformer model for Masked Language Modeling."""
    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):
        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim, rngs=rngs)
        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, feed_forward_dim, rngs=rngs) for _ in range(num_transformer_blocks)]
        self.output_layer = nnx.Linear(
            in_features=embed_dim, out_features=vocab_size,
            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),
            bias_init=nnx.with_partitioning(nnx.initializers.zeros_init(), NamedSharding(mesh, P('model'))),
            use_bias=False, rngs=rngs
        )

    def __call__(self, inputs, training: bool = False):
        x = self.embedding_layer(inputs)
        for block in self.transformer_blocks:
            x = block(x, training=training)
        return self.output_layer(x)

    def embed(self, inputs, training: bool = False):
        """Gets embeddings before the final output layer for MTEB."""
        x = self.embedding_layer(inputs)
        for block in self.transformer_blocks:
            x = block(x, training=training)
        return x

def create_model(rngs, config):
    """Creates the MiniBERT model."""
    return MiniBERT(
        maxlen=config['maxlen'], vocab_size=config['vocab_size'], embed_dim=config['embed_dim'],
        num_heads=config['num_heads'], feed_forward_dim=config['feed_forward_dim'],
        num_transformer_blocks=config['num_transformer_blocks'], rngs=rngs
    )

# --- Data Preprocessing ---
def create_masked_lm_predictions(tokens, mask_prob, maxlen, vocab_size):
    labels = np.full(maxlen, -100)
    non_padding_indices = np.where(tokens != 0)[0]
    if len(non_padding_indices) == 0: return tokens, labels
    num_to_predict = max(1, int(round(len(non_padding_indices) * mask_prob)))
    masked_indices = np.random.choice(non_padding_indices, size=min(num_to_predict, len(non_padding_indices)), replace=False)
    labels[masked_indices] = tokens[masked_indices]
    for i in masked_indices:
        rand = np.random.rand()
        if rand < 0.8: tokens[i] = MASK_TOKEN_ID
        elif rand < 0.9: tokens[i] = np.random.randint(0, vocab_size)
    return tokens, labels

def process_dataset_for_mlm(dataset, maxlen, mask_prob, vocab_size):
    def tokenize_pad_and_mask(examples):
        input_ids, labels = [], []
        for text in examples['text']:
            tokens = tokenizer.encode(text, allowed_special={'<|endoftext|>'})[:maxlen]
            padded = np.array(tokens + [0] * (maxlen - len(tokens)))
            masked, label = create_masked_lm_predictions(padded.copy(), mask_prob, maxlen, vocab_size)
            input_ids.append(masked.tolist())
            labels.append(label.tolist())
        return {'input_ids': input_ids, 'labels': labels}
    columns_to_remove = [col for col in dataset.column_names if col not in ['input_ids', 'labels']]
    dataset = dataset.map(tokenize_pad_and_mask, batched=True, batch_size=1000, remove_columns=columns_to_remove)
    return dataset.shuffle(buffer_size=10_000, seed=42)

# --- JAX Loss and Step Functions ---
def loss_fn(model, batch, training: bool):
    logits = model(batch['input_ids'], training=training)
    labels = batch['labels']
    logits_flat, labels_flat = logits.reshape(-1, logits.shape[-1]), labels.reshape(-1)
    loss_per_pos = optax.softmax_cross_entropy_with_integer_labels(logits=logits_flat, labels=labels_flat)
    num_masked = jnp.sum(labels_flat != -100)
    return jnp.where(num_masked > 0, jnp.sum(loss_per_pos) / num_masked, 0.0), logits

@nnx.jit
def train_step(model: MiniBERT, optimizer: nnx.Optimizer, batch):
    grad_fn = nnx.value_and_grad(lambda m, b: loss_fn(m, b, training=True), has_aux=True)
    (loss, _), grads = grad_fn(model, batch)
    optimizer.update(grads)
    return loss, model, optimizer

@nnx.jit
def eval_step(model: MiniBERT, batch):
    loss, _ = loss_fn(model, batch, training=False)
    return loss

# --- MTEB Wrapper ---
class MiniBERTForMTEB:
    """Wrapper class for MTEB compatibility."""
    def __init__(self, model: MiniBERT, tokenizer, maxlen: int, batch_size: int = 32):
        self.model = model
        self.tokenizer = tokenizer
        self.maxlen = maxlen
        self.batch_size = batch_size
        self._embed_fn = jax.jit(self.model.embed, static_argnames='training')

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output
        mask = np.expand_dims(attention_mask, -1).astype(np.float32)
        sum_embeddings = np.sum(token_embeddings * mask, 1)
        sum_mask = np.clip(mask.sum(1), a_min=1e-9, a_max=None)
        return sum_embeddings / sum_mask

    def encode(self, sentences, **kwargs):
        all_embeddings = []
        for i in range(0, len(sentences), self.batch_size):
            batch = sentences[i:i + self.batch_size]
            inputs = self.tokenizer.encode_batch(batch)
            ids = np.zeros((len(batch), self.maxlen), dtype=np.int32)
            mask = np.zeros((len(batch), self.maxlen), dtype=np.int32)
            for j, encoded in enumerate(inputs):
                seq_len = min(len(encoded), self.maxlen)
                ids[j, :seq_len], mask[j, :seq_len] = encoded[:seq_len], 1

            hidden_state = self._embed_fn(jax.device_put(ids), training=False)
            pooled = self._mean_pooling(np.asarray(hidden_state), mask)
            norms = np.linalg.norm(pooled, axis=1, keepdims=True)
            all_embeddings.append(pooled / (norms + 1e-12))
        return np.vstack(all_embeddings)

# --- Main Functions ---
def main_pretrain():
    """Runs the pre-training loop."""
    config = {
        'vocab_size': tokenizer.n_vocab + 1, 'num_transformer_blocks': 12, 'maxlen': 512,
        'embed_dim': 768, 'num_heads': 12, 'feed_forward_dim': 3072, 'batch_size': 32,
        'learning_rate': 1e-4, 'mask_prob': 0.15,
        # Increased tokens to allow for meaningful training.
        # WARNING: This will take a very long time on standard hardware.
        'max_tokens_to_process': 1_000_000_000,
        'eval_interval': 1000, 'eval_steps': 100, 'val_set_size': 2000,
        # Adjusted checkpoint interval for a longer run.
        'checkpoint_interval': 10000, 'checkpoint_dir': './minibert_checkpoints',
        'wandb_project': 'fineweb-bert-pretraining-combined'
    }
    # Convert checkpoint directory to an absolute path to satisfy orbax requirement
    config['checkpoint_dir'] = os.path.abspath(config['checkpoint_dir'])
    os.makedirs(config['checkpoint_dir'], exist_ok=True)

    max_iterations = config['max_tokens_to_process'] // (config['batch_size'] * config['maxlen'])
    last_checkpoint_path = ""

    wandb.init(project=config['wandb_project'], config=config)
    rngs = nnx.Rngs(0)
    model = create_model(rngs, config)
    optimizer = nnx.Optimizer(model, optax.adamw(config['learning_rate']))

    print("Loading and processing dataset...")
    full_dataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)
    train_dataset = process_dataset_for_mlm(full_dataset.skip(config['val_set_size']), config['maxlen'], config['mask_prob'], config['vocab_size'])
    val_dataset = process_dataset_for_mlm(full_dataset.take(config['val_set_size']), config['maxlen'], config['mask_prob'], config['vocab_size'])
    train_iterator = iter(train_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))
    val_iterator = iter(val_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))

    print(f"--- Starting Pre-training for {max_iterations} steps ---")
    start_time = time.time()
    for step in range(max_iterations):
        try:
            batch = next(train_iterator)
        except StopIteration:
            train_iterator = iter(train_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))
            batch = next(train_iterator)

        sharding = NamedSharding(mesh, P('batch', None))
        sharded_batch = {k: jax.device_put(jnp.array(v), sharding) for k, v in batch.items()}
        loss, model, optimizer = train_step(model, optimizer, sharded_batch)
        wandb.log({"train_loss": loss.item()}, step=step)

        if (step + 1) % config['eval_interval'] == 0:
            val_losses = []
            for _ in range(config['eval_steps']):
                try: val_batch = next(val_iterator)
                except StopIteration:
                    val_iterator = iter(val_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))
                    break
                sharded_val_batch = {k: jax.device_put(jnp.array(v), sharding) for k,v in val_batch.items()}
                val_losses.append(eval_step(model, sharded_val_batch))
            if val_losses:
                avg_val_loss = jnp.mean(jnp.array(val_losses))
                wandb.log({"val_loss": avg_val_loss.item()}, step=step)
                print(f"Step {step+1}/{max_iterations}, Val Loss: {avg_val_loss.item():.4f}, Time: {time.time()-start_time:.2f}s")
                start_time = time.time()

        if (step + 1) % config['checkpoint_interval'] == 0:
            path = os.path.join(config['checkpoint_dir'], f'step_{step+1}')
            # Create a new checkpointer for each save and close it to ensure finalization.
            checkpointer = orbax.PyTreeCheckpointer()
            # nnx.split returns graphdef, plus one state per filter.
            # We expect 3 values: graphdef, params, and the rest (...).
            _, param_state, _ = nnx.split(model, nnx.Param, ...)
            # Save only the serializable parameters.
            checkpointer.save(path, item=param_state)
            checkpointer.close() # This blocks until the save is fully complete.
            last_checkpoint_path = path
            print(f"Checkpoint saved at {last_checkpoint_path}")

    # Do not finish wandb run here, let it continue into the evaluation phase.
    return last_checkpoint_path, config

def main_eval(checkpoint_path, config):
    """Loads a model checkpoint and runs MTEB evaluation."""
    print(f"\n--- Loading model from {checkpoint_path} for MTEB evaluation ---")
    rngs = nnx.Rngs(0)
    # Create a new model instance. This will be our shell with the correct structure and initial RNG state.
    restored_model = create_model(rngs, config)

    # Create a PyTree template containing only the Parameters to guide the restore process.
    # Unpack all 3 values: graphdef, the params template, and the rest.
    _, params_template, _ = nnx.split(restored_model, nnx.Param, ...)

    checkpointer = orbax.PyTreeCheckpointer()

    # Restore the saved parameters using the template.
    restored_params = checkpointer.restore(checkpoint_path, item=params_template)

    if restored_params is None:
        print("Error: Could not restore parameters. Aborting evaluation.")
        return

    # Update the model in-place with the restored parameters.
    nnx.update(restored_model, restored_params)

    print("Model loaded successfully.")

    mteb_model = MiniBERTForMTEB(model=restored_model, tokenizer=tokenizer, maxlen=config['maxlen'])
    # Using the newer version of Banking77Classification as recommended by the MTEB warning.
    tasks = ["STSBenchmark", "Banking77Classification.v2", "FiQA2018"]
    evaluation = MTEB(tasks=tasks)
    print(f"Starting MTEB evaluation on: {tasks}")
    results = evaluation.run(mteb_model, output_folder="mteb_results", eval_splits=["test"])
    print("\n--- MTEB Evaluation Finished ---")

    # Pretty-print the results for better readability
    print("\n--- MTEB Evaluation Results ---")

    # Log the evaluation results to Weights & Biases
    print("\n--- Logging MTEB results to Weights & Biases ---")
    eval_results_for_wandb = {}
    for task_result in results:
        task_name = task_result.task_name
        print(f"\nTask: {task_result.task_name}")
        print(json.dumps(task_result.scores, indent=2))
        # Extract the main score for logging
        if task_result.scores and 'test' in task_result.scores and task_result.scores['test']:
            test_scores = task_result.scores['test'][0]
            if 'main_score' in test_scores:
                eval_results_for_wandb[f"eval/{task_name}/main_score"] = test_scores['main_score']

    if eval_results_for_wandb:
        wandb.log(eval_results_for_wandb)
        print("Successfully logged evaluation results to WandB.")


if __name__ == '__main__':
    # Login to wandb first using `wandb login` in your terminal
    final_checkpoint, training_config = main_pretrain()

    if final_checkpoint:
        main_eval(final_checkpoint, training_config)
    else:
        print("\n--- Pre-training did not produce a checkpoint. Skipping evaluation. ---")

    # Finish the W&B run after both training and evaluation are complete.
    wandb.finish()

