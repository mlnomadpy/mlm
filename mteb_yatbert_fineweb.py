# -*- coding: utf-8 -*-
"""MTEB YatBERT - Fineweb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/#fileId=https%3A//storage.googleapis.com/kaggle-colab-exported-notebooks/skywolfmo/mteb-yatbert-fineweb.fa1bdead-b210-4a72-bdf4-3e854beb732d.ipynb%3FX-Goog-Algorithm%3DGOOG4-RSA-SHA256%26X-Goog-Credential%3Dgcp-kaggle-com%2540kaggle-161607.iam.gserviceaccount.com/20250913/auto/storage/goog4_request%26X-Goog-Date%3D20250913T024802Z%26X-Goog-Expires%3D259200%26X-Goog-SignedHeaders%3Dhost%26X-Goog-Signature%3Da6adcfa8656c53bd500bad8fe91b921b44bb880ee7e1dc51e0752b23e6ba4d408fdc0bd9c3bbc9f9a4a6478dcee72363f977239769786d40820310e579cdc9a425b2cf4dc6c5f8a9e4f9593f4c6d1d9a84553475b2ad8faeaf4536d8cd3d5b0f7dc8bfd0e06149e9efff44d240eb3c454d4fbcf8258734b024c0a75e64792ae605608f0ff35cefc734fc9cf5e21ce1968e5b3aeda15dfa74b9c35efe8bf13d3bacf7684456c6da9a8afd8201cf607802aa3d5beddc2dd5597841cb58c6b7785ff88dc4f9c4ec6b06501da806ed9dcba73ac9286414d9f3102cb9fe40d16e996f41d21a398b10f1634055a57cefa41c0cad1fef5bb473b58f25ff3ccd38ba05c4
"""

!pip install -Uq tiktoken datasets wandb mteb nmn

from kaggle_secrets import UserSecretsClient
import os
user_secrets = UserSecretsClient()
os.environ["HF_TOKEN"] = user_secrets.get_secret("HF_TOKEN")
os.environ["WANDB_API_KEY"] = user_secrets.get_secret("WANDB_API_KEY")
os.environ["WANDB_ENTITY"] = user_secrets.get_secret("WANDB_ENTITY")

# Commented out IPython magic to ensure Python compatibility.
# %%writefile /usr/local/lib/python3.10/site-packages/flax/typing.py
# # Copyright 2024 The Flax Authors.
# #
# # Licensed under the Apache License, Version 2.0 (the "License");
# # you may not use this file except in compliance with the License.
# # You may obtain a copy of the License at
# #
# #     http://www.apache.org/licenses/LICENSE-2.0
# #
# # Unless required by applicable law or agreed to in writing, software
# # distributed under the License is distributed on an "AS IS" BASIS,
# # WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# # See the License for the specific language governing permissions and
# # limitations under the License.
# from __future__ import annotations
# 
# from collections import deque
# from functools import partial
# from typing import (
#   Any,
#   Generic,
#   Optional,
#   Protocol,
#   TypeGuard,
#   TypeVar,
#   Union,
# )
# from collections.abc import Callable, Hashable, Mapping, Sequence
# 
# import jax
# import jax.numpy as jnp
# import numpy as np
# from flax.core import FrozenDict
# 
# import dataclasses
# import jax.tree_util as jtu
# 
# 
# # General
# 
# Array = Union[jax.Array, Any]
# PRNGKey = jax.Array
# RNGSequences = dict[str, PRNGKey]
# Dtype = Union[jax.typing.DTypeLike, Any]
# Shape = Sequence[int]
# K = TypeVar('K')
# 
# class Key(Hashable, Protocol):
#   def __lt__(self: K, value: K, /) -> bool:
#     ...
# 
# def is_key_like(x: Any) -> TypeGuard[Key]:
#   return hasattr(x, '__hash__') and hasattr(x, '__lt__')
# 
# Path = str
# PathParts = tuple[Key, ...]
# 
# Leaf = Any
# 
# 
# # Linear
# 
# PrecisionLike = Union[
#   None,
#   str,
#   jax.lax.Precision,
#   tuple[str, str],
#   tuple[jax.lax.Precision, jax.lax.Precision],
# ]
# DotGeneralT = Callable[..., Array]
# ConvGeneralDilatedT = Callable[..., Array]
# EinsumT = Callable[..., Array]
# 
# PaddingLike = Union[str, int, Sequence[Union[int, tuple[int, int]]]]
# LaxPadding = Union[str, Sequence[tuple[int, int]]]
# 
# 
# # Initializers
# 
# Initializer = Union[jax.nn.initializers.Initializer, Callable[..., Any]]
# 
# 
# # Collections
# 
# Collection = Mapping[str, Any]
# MutableCollection = dict[str, Any]
# 
# 
# # Dicts
# 
# VariableDict = Mapping[str, Collection]
# FrozenVariableDict = FrozenDict[str, Collection]
# MutableVariableDict = dict[str, MutableCollection]
# 
# PRNGFoldable = Union[int, str]
# 
# 
# # Axes
# 
# T = TypeVar('T')
# 
# @dataclasses.dataclass(frozen=True)
# class In(Generic[T]):
#   """Specifies a variable collection should only be lifted as input."""
# 
#   axis: T
# 
# @dataclasses.dataclass(frozen=True)
# class Out(Generic[T]):
#   """Specifies a variable collection should only be lifted as output."""
# 
#   axis: T
# 
# Axis = Optional[int]
# InOutAxis = Union[Axis, In[Axis], Out[Axis]]
# 
# ScanAxis = int
# InOutScanAxis = Union[ScanAxis, In[ScanAxis], Out[ScanAxis]]
# 
# Axes = Union[int, Sequence[int]]
# 
# 
# # SPMD
# 
# LogicalNames = tuple[Union[str, None], ...]
# AxisName = str | tuple[str, ...] | None
# 
# # Maps each logical axis  to physical mesh, can be either None (replicated),
# # one physical axis or a tuple of physical axes.
# LogicalRules = Sequence[tuple[str, AxisName]]
# ArrayPytree = Any  # pylint: disable=invalid-name
# LogicalPartitionSpec = Any  # pylint: disable=invalid-name
# LogicalPartitionSpecPytree = Any  # pylint: disable=invalid-name
# PartitionSpecPytree = Any  # pylint: disable=invalid-name
# 
# Sharding = tuple[AxisName, ...]
# 
# A = TypeVar('A')
# 
# 
# class PytreeDeque(deque[A]):
#   pass
# 
# 
# def _pytree_deque_flatten(xs: PytreeDeque, *, with_path: bool):
#   if with_path:
#     nodes = tuple((jtu.SequenceKey(i), x) for i, x in enumerate(xs))
#     return nodes, ()
#   else:
#     return xs, ()
# 
# 
# def _pytree_deque_unflatten(_, nodes):
#   return PytreeDeque(nodes)
# 
# 
# jtu.register_pytree_with_keys(
#   PytreeDeque,
#   partial(_pytree_deque_flatten, with_path=True),
#   _pytree_deque_unflatten,
#   flatten_func=partial(_pytree_deque_flatten, with_path=False),
# )
# 
# class Missing:
#   pass
# 
# 
# MISSING = Missing()
# 
# 
# def _bytes_repr(num_bytes):
#   count, units = (
#     (f'{num_bytes / 1e9:,.1f}', 'GB')
#     if num_bytes > 1e9
#     else (f'{num_bytes / 1e6:,.1f}', 'MB')
#     if num_bytes > 1e6
#     else (f'{num_bytes / 1e3:,.1f}', 'KB')
#     if num_bytes > 1e3
#     else (f'{num_bytes:,}', 'B')
#   )
# 
#   return f'{count} {units}'
# 
# 
# class ShapeDtype(Protocol):
#   shape: Shape
#   dtype: Dtype
# 
# 
# def has_shape_dtype(x: Any) -> TypeGuard[ShapeDtype]:
#   return hasattr(x, 'shape') and hasattr(x, 'dtype')
# 
# 
# @dataclasses.dataclass(frozen=True, slots=True)
# class SizeBytes:  # type: ignore[misc]
#   size: int
#   bytes: int
# 
#   @classmethod
#   def from_array(cls, x: ShapeDtype):
#     size = int(np.prod(x.shape))
#     dtype: jnp.dtype
#     if isinstance(x.dtype, str):
#       dtype = jnp.dtype(x.dtype)
#     else:
#       dtype = x.dtype  # type: ignore
#     bytes = size * dtype.itemsize  # type: ignore
#     return cls(size, bytes)
# 
#   def __add__(self, other: SizeBytes):
#     return type(self)(self.size + other.size, self.bytes + other.bytes)
# 
#   def __bool__(self) -> bool:
#     return bool(self.size)
# 
#   def __repr__(self) -> str:
#     bytes_repr = _bytes_repr(self.bytes)
#     return f'{self.size:,} ({bytes_repr})'
# 
#   @classmethod
#   def from_any(cls, x):
#     leaves = jax.tree.leaves(x)
#     size_bytes = cls(0, 0)
#     for leaf in leaves:
#       if has_shape_dtype(leaf):
#         size_bytes += cls.from_array(leaf)
# 
#     return size_bytes
# 
# 
# TupleArg = TypeVar('TupleArg', bound=tuple)
# 
# 
# class PromoteDtypeFn(Protocol):
#   def __call__(
#     self, args: TupleArg, /, *, dtype: Any = None, inexact: bool = True
#   ) -> TupleArg: ...
#

!rm -rf /kaggle/working/minibert_checkpoints

# Install necessary libraries
# The following command ensures JAX is installed correctly for a TPU environment,
# which is critical to avoid XlaRuntimeError. It also installs/upgrades other dependencies.
# !pip install -Uq "jax[tpu]" -f https://storage.googleapis.com/jax-releases/libtpu_releases.html
# !pip install -Uq tiktoken datasets wandb orbax-checkpoint flax optax mteb torch

import os
import time
import json
from dataclasses import dataclass

import flax.nnx as nnx
import jax
import jax.numpy as jnp
import numpy as np
import optax
import orbax.checkpoint as orbax
import tiktoken
import wandb
from datasets import load_dataset
from jax.experimental import mesh_utils
from jax.sharding import Mesh, NamedSharding
from jax.sharding import PartitionSpec as P
from mteb import MTEB
from nmn.nnx.nmn import YatNMN

# --- JAX Device and Mesh Setup ---
# Initialize JAX devices
if jax.default_backend() == 'tpu':
    mesh = Mesh(mesh_utils.create_device_mesh((4, 2)), ('batch', 'model'))
else:
    num_devices = len(jax.devices())
    mesh_shape = (num_devices, 1)
    mesh = Mesh(mesh_utils.create_device_mesh(mesh_shape), ('batch', 'model'))

# --- Tokenizer ---
tokenizer = tiktoken.get_encoding("gpt2")
MASK_TOKEN_ID = tokenizer.n_vocab


# --- Model Definition ---
class TransformerBlock(nnx.Module):
    """A standard Transformer block with bidirectional self-attention."""
    def __init__(self, embed_dim: int, num_heads: int, ff_dim: int, *, rngs: nnx.Rngs, rate: float = 0.1):
        if mesh is not None:
            kernel_init = nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model')))
        else:
            kernel_init = nnx.initializers.xavier_uniform()

        self.mha = nnx.MultiHeadAttention(num_heads=num_heads, in_features=embed_dim, use_bias=False, kernel_init=kernel_init, rngs=rngs)
        self.dropout1 = nnx.Dropout(rate=rate, rngs=rngs)
        self.ffn = nnx.Sequential(
            YatNMN(embed_dim, ff_dim, kernel_init=kernel_init, rngs=rngs),
            nnx.Linear(ff_dim, embed_dim, kernel_init=kernel_init, rngs=rngs)
        )
        self.dropout2 = nnx.Dropout(rate=rate, rngs=rngs)
        self.norm1 = nnx.LayerNorm(embed_dim, rngs=rngs)
        self.norm2 = nnx.LayerNorm(embed_dim, rngs=rngs)

    def __call__(self, inputs, training: bool = False):
        x = self.norm1(inputs)
        attention_output = self.mha(inputs_q=x, decode=False)
        attention_output = self.dropout1(attention_output, deterministic=not training)
        out1 = inputs + attention_output
        x = self.norm2(out1)
        ffn_output = self.ffn(x)
        ffn_output = self.dropout2(ffn_output, deterministic=not training)
        return out1 + ffn_output


class TokenAndPositionEmbedding(nnx.Module):
    """Combines token and positional embeddings."""
    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, *, rngs: nnx.Rngs):
        self.token_emb = nnx.Embed(num_embeddings=vocab_size, features=embed_dim, rngs=rngs)
        self.pos_emb = nnx.Embed(num_embeddings=maxlen, features=embed_dim, rngs=rngs)

    def __call__(self, x):
        positions = jnp.arange(0, x.shape[1])[None, :]
        return self.token_emb(x) + self.pos_emb(positions)

class MiniBERT(nnx.Module):
    """A miniBERT transformer model for Masked Language Modeling."""
    def __init__(self, maxlen: int, vocab_size: int, embed_dim: int, num_heads: int, feed_forward_dim: int, num_transformer_blocks: int, rngs: nnx.Rngs):
        self.embedding_layer = TokenAndPositionEmbedding(maxlen, vocab_size, embed_dim, rngs=rngs)
        self.transformer_blocks = [TransformerBlock(embed_dim, num_heads, feed_forward_dim, rngs=rngs) for _ in range(num_transformer_blocks)]
        self.output_layer = nnx.Linear(
            in_features=embed_dim, out_features=vocab_size,
            kernel_init=nnx.with_partitioning(nnx.initializers.xavier_uniform(), NamedSharding(mesh, P(None, 'model'))),
            use_bias=False,
            rngs=rngs
        )

    def __call__(self, inputs, training: bool = False):
        x = self.embedding_layer(inputs)
        for block in self.transformer_blocks:
            x = block(x, training=training)
        return self.output_layer(x)

    def embed(self, inputs, training: bool = False):
        """Gets embeddings before the final output layer for MTEB."""
        x = self.embedding_layer(inputs)
        for block in self.transformer_blocks:
            x = block(x, training=training)
        return x

def create_model(rngs, config):
    """Creates the MiniBERT model."""
    return MiniBERT(
        maxlen=config['maxlen'], vocab_size=config['vocab_size'], embed_dim=config['embed_dim'],
        num_heads=config['num_heads'], feed_forward_dim=config['feed_forward_dim'],
        num_transformer_blocks=config['num_transformer_blocks'], rngs=rngs
    )

# --- Data Preprocessing ---
def create_masked_lm_predictions(tokens, mask_prob, maxlen, vocab_size):
    labels = np.full(maxlen, -100)
    non_padding_indices = np.where(tokens != 0)[0]
    if len(non_padding_indices) == 0: return tokens, labels
    num_to_predict = max(1, int(round(len(non_padding_indices) * mask_prob)))
    masked_indices = np.random.choice(non_padding_indices, size=min(num_to_predict, len(non_padding_indices)), replace=False)
    labels[masked_indices] = tokens[masked_indices]
    for i in masked_indices:
        rand = np.random.rand()
        if rand < 0.8: tokens[i] = MASK_TOKEN_ID
        elif rand < 0.9: tokens[i] = np.random.randint(0, vocab_size)
    return tokens, labels

def process_dataset_for_mlm(dataset, maxlen, mask_prob, vocab_size):
    def tokenize_pad_and_mask(examples):
        input_ids, labels = [], []
        for text in examples['text']:
            tokens = tokenizer.encode(text, allowed_special={'<|endoftext|>'})[:maxlen]
            padded = np.array(tokens + [0] * (maxlen - len(tokens)))
            masked, label = create_masked_lm_predictions(padded.copy(), mask_prob, maxlen, vocab_size)
            input_ids.append(masked.tolist())
            labels.append(label.tolist())
        return {'input_ids': input_ids, 'labels': labels}
    columns_to_remove = [col for col in dataset.column_names if col not in ['input_ids', 'labels']]
    dataset = dataset.map(tokenize_pad_and_mask, batched=True, batch_size=1000, remove_columns=columns_to_remove)
    return dataset.shuffle(buffer_size=10_000, seed=42)

# --- JAX Loss and Step Functions ---
def loss_fn(model, batch, training: bool):
    logits = model(batch['input_ids'], training=training)
    labels = batch['labels']
    logits_flat, labels_flat = logits.reshape(-1, logits.shape[-1]), labels.reshape(-1)
    loss_per_pos = optax.softmax_cross_entropy_with_integer_labels(logits=logits_flat, labels=labels_flat)
    num_masked = jnp.sum(labels_flat != -100)
    return jnp.where(num_masked > 0, jnp.sum(loss_per_pos) / num_masked, 0.0), logits

@nnx.jit
def train_step(model: MiniBERT, optimizer: nnx.Optimizer, batch):
    grad_fn = nnx.value_and_grad(lambda m, b: loss_fn(m, b, training=True), has_aux=True)
    (loss, _), grads = grad_fn(model, batch)
    optimizer.update(grads)
    return loss, model, optimizer

@nnx.jit
def eval_step(model: MiniBERT, batch):
    loss, _ = loss_fn(model, batch, training=False)
    return loss

# --- MTEB Wrapper ---
class MiniBERTForMTEB:
    """Wrapper class for MTEB compatibility."""
    def __init__(self, model: MiniBERT, tokenizer, maxlen: int, batch_size: int = 32):
        self.model = model
        self.tokenizer = tokenizer
        self.maxlen = maxlen
        self.batch_size = batch_size
        self._embed_fn = jax.jit(self.model.embed, static_argnames='training')

    def _mean_pooling(self, model_output, attention_mask):
        token_embeddings = model_output
        mask = np.expand_dims(attention_mask, -1).astype(np.float32)
        sum_embeddings = np.sum(token_embeddings * mask, 1)
        sum_mask = np.clip(mask.sum(1), a_min=1e-9, a_max=None)
        return sum_embeddings / sum_mask

    def encode(self, sentences, **kwargs):
        all_embeddings = []
        for i in range(0, len(sentences), self.batch_size):
            batch = sentences[i:i + self.batch_size]
            inputs = self.tokenizer.encode_batch(batch)
            ids = np.zeros((len(batch), self.maxlen), dtype=np.int32)
            mask = np.zeros((len(batch), self.maxlen), dtype=np.int32)
            for j, encoded in enumerate(inputs):
                seq_len = min(len(encoded), self.maxlen)
                ids[j, :seq_len], mask[j, :seq_len] = encoded[:seq_len], 1

            hidden_state = self._embed_fn(jax.device_put(ids), training=False)
            pooled = self._mean_pooling(np.asarray(hidden_state), mask)
            norms = np.linalg.norm(pooled, axis=1, keepdims=True)
            all_embeddings.append(pooled / (norms + 1e-12))
        return np.vstack(all_embeddings)

# --- Main Functions ---
def main_pretrain():
    """Runs the pre-training loop."""
    config = {
        'vocab_size': tokenizer.n_vocab + 1, 'num_transformer_blocks': 12, 'maxlen': 512,
        'embed_dim': 768, 'num_heads': 12, 'feed_forward_dim': 3072, 'batch_size': 32,
        'learning_rate': 1e-4, 'mask_prob': 0.15,
        # Increased tokens to allow for meaningful training.
        # WARNING: This will take a very long time on standard hardware.
        'max_tokens_to_process': 1_000_000_000,
        'eval_interval': 1000, 'eval_steps': 100, 'val_set_size': 2000,
        # Adjusted checkpoint interval for a longer run.
        'checkpoint_interval': 10000, 'checkpoint_dir': './minibert_checkpoints',
        'wandb_project': 'fineweb-bert-pretraining-combined'
    }
    # Convert checkpoint directory to an absolute path to satisfy orbax requirement
    config['checkpoint_dir'] = os.path.abspath(config['checkpoint_dir'])
    os.makedirs(config['checkpoint_dir'], exist_ok=True)

    max_iterations = config['max_tokens_to_process'] // (config['batch_size'] * config['maxlen'])
    last_checkpoint_path = ""

    wandb.init(project=config['wandb_project'], config=config)
    rngs = nnx.Rngs(0)
    model = create_model(rngs, config)
    optimizer = nnx.Optimizer(model, optax.adamw(config['learning_rate']))

    print("Loading and processing dataset...")
    full_dataset = load_dataset('HuggingFaceFW/fineweb', split='train', streaming=True)
    train_dataset = process_dataset_for_mlm(full_dataset.skip(config['val_set_size']), config['maxlen'], config['mask_prob'], config['vocab_size'])
    val_dataset = process_dataset_for_mlm(full_dataset.take(config['val_set_size']), config['maxlen'], config['mask_prob'], config['vocab_size'])
    train_iterator = iter(train_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))
    val_iterator = iter(val_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))

    print(f"--- Starting Pre-training for {max_iterations} steps ---")
    start_time = time.time()
    for step in range(max_iterations):
        try:
            batch = next(train_iterator)
        except StopIteration:
            train_iterator = iter(train_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))
            batch = next(train_iterator)

        sharding = NamedSharding(mesh, P('batch', None))
        sharded_batch = {k: jax.device_put(jnp.array(v), sharding) for k, v in batch.items()}
        loss, model, optimizer = train_step(model, optimizer, sharded_batch)
        wandb.log({"train_loss": loss.item()}, step=step)

        if (step + 1) % config['eval_interval'] == 0:
            val_losses = []
            for _ in range(config['eval_steps']):
                try: val_batch = next(val_iterator)
                except StopIteration:
                    val_iterator = iter(val_dataset.iter(batch_size=config['batch_size'], drop_last_batch=True))
                    break
                sharded_val_batch = {k: jax.device_put(jnp.array(v), sharding) for k,v in val_batch.items()}
                val_losses.append(eval_step(model, sharded_val_batch))
            if val_losses:
                avg_val_loss = jnp.mean(jnp.array(val_losses))
                wandb.log({"val_loss": avg_val_loss.item()}, step=step)
                print(f"Step {step+1}/{max_iterations}, Val Loss: {avg_val_loss.item():.4f}, Time: {time.time()-start_time:.2f}s")
                start_time = time.time()

        if (step + 1) % config['checkpoint_interval'] == 0:
            path = os.path.join(config['checkpoint_dir'], f'step_{step+1}')
            # Create a new checkpointer for each save and close it to ensure finalization.
            checkpointer = orbax.PyTreeCheckpointer()
            # nnx.split returns graphdef, plus one state per filter.
            # We expect 3 values: graphdef, params, and the rest (...).
            _, param_state, _ = nnx.split(model, nnx.Param, ...)
            # Save only the serializable parameters.
            checkpointer.save(path, item=param_state)
            checkpointer.close() # This blocks until the save is fully complete.
            last_checkpoint_path = path
            print(f"Checkpoint saved at {last_checkpoint_path}")

    # Do not finish wandb run here, let it continue into the evaluation phase.
    return last_checkpoint_path, config

def main_eval(checkpoint_path, config):
    """Loads a model checkpoint and runs MTEB evaluation."""
    print(f"\n--- Loading model from {checkpoint_path} for MTEB evaluation ---")
    rngs = nnx.Rngs(0)
    # Create a new model instance. This will be our shell with the correct structure and initial RNG state.
    restored_model = create_model(rngs, config)

    # Create a PyTree template containing only the Parameters to guide the restore process.
    # Unpack all 3 values: graphdef, the params template, and the rest.
    _, params_template, _ = nnx.split(restored_model, nnx.Param, ...)

    checkpointer = orbax.PyTreeCheckpointer()

    # Restore the saved parameters using the template.
    restored_params = checkpointer.restore(checkpoint_path, item=params_template)

    if restored_params is None:
        print("Error: Could not restore parameters. Aborting evaluation.")
        return

    # Update the model in-place with the restored parameters.
    nnx.update(restored_model, restored_params)

    print("Model loaded successfully.")

    mteb_model = MiniBERTForMTEB(model=restored_model, tokenizer=tokenizer, maxlen=config['maxlen'])
    # Using the newer version of Banking77Classification as recommended by the MTEB warning.
    tasks = ["STSBenchmark", "Banking77Classification.v2", "FiQA2018"]
    evaluation = MTEB(tasks=tasks)
    print(f"Starting MTEB evaluation on: {tasks}")
    results = evaluation.run(mteb_model, output_folder="mteb_results", eval_splits=["test"])
    print("\n--- MTEB Evaluation Finished ---")

    # Pretty-print the results for better readability
    print("\n--- MTEB Evaluation Results ---")

    # Log the evaluation results to Weights & Biases
    print("\n--- Logging MTEB results to Weights & Biases ---")
    eval_results_for_wandb = {}
    for task_result in results:
        task_name = task_result.task_name
        print(f"\nTask: {task_result.task_name}")
        print(json.dumps(task_result.scores, indent=2))
        # Extract the main score for logging
        if task_result.scores and 'test' in task_result.scores and task_result.scores['test']:
            test_scores = task_result.scores['test'][0]
            if 'main_score' in test_scores:
                eval_results_for_wandb[f"eval/{task_name}/main_score"] = test_scores['main_score']

    if eval_results_for_wandb:
        wandb.log(eval_results_for_wandb)
        print("Successfully logged evaluation results to WandB.")


if __name__ == '__main__':
    # Login to wandb first using `wandb login` in your terminal
    final_checkpoint, training_config = main_pretrain()

    if final_checkpoint:
        main_eval(final_checkpoint, training_config)
    else:
        print("\n--- Pre-training did not produce a checkpoint. Skipping evaluation. ---")

    # Finish the W&B run after both training and evaluation are complete.
    wandb.finish()

